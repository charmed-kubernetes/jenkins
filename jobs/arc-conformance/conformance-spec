#!/bin/bash
# shellcheck disable=SC2034,SC1090

set -ex

###############################################################################
# INITIALIZE
###############################################################################
: "${WORKSPACE:=$(pwd)}"

. "$WORKSPACE/ci.bash"
. "$WORKSPACE/juju.bash"

###############################################################################
# FUNCTION OVERRIDES
###############################################################################
function juju::bootstrap::before
{
    # azure-cli
    which az || sudo apt-get install -qyf azure-cli
    rm -rf ${HOME}/.azure/cliextensions/*

    # can't get to az extension index from jenkins worker, so install the bundled extensions
    az extension add -y --source ./jobs/arc-conformance/connectedk8s-1.2.8-py2.py3-none-any.whl --pip-proxy http://squid.internal:3128
    az version || exit 1

    # helm needs to be < 3.7.0 for:
    #  https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/troubleshooting#azure-cli-is-unable-to-download-helm-chart-for-azure-arc-agents
    if snap list helm 2>/dev/null; then
        sudo snap refresh helm --channel=3.6/stable
    else
        sudo snap install helm --classic --channel=3.6/stable
    fi
    helm version || exit 1

    # sonobuoy
    rm -rf sonobuoy*
    SB_FILE="sonobuoy_${SONOBUOY_VERSION}_linux_${ARCH}.tar.gz"
    wget https://github.com/vmware-tanzu/sonobuoy/releases/download/v${SONOBUOY_VERSION}/${SB_FILE}
    tar xvf ${SB_FILE}
    rm -f ${SB_FILE}
    ./sonobuoy version || exit 1
}

function juju::bootstrap
{
    # override because azure needs image-stream=released
    juju bootstrap "$JUJU_CLOUD" "$JUJU_CONTROLLER" \
         -d "$JUJU_MODEL" \
         --bootstrap-series "$SERIES" \
         --force \
         --bootstrap-constraints arch="$ARCH" \
         --model-default test-mode=true \
         --model-default resource-tags="owner=k8sci" \
         --model-default image-stream=released \
         --model-default automatically-retry-hooks=true \
         --model-default logging-config="<root>=DEBUG"

    ret=$?
    if (( ret > 0 )); then
        exit "$ret"
    fi
}

function juju::deploy::overlay
{
    # override to set allow-privileged for sonobuoy
    cat <<EOF > overlay.yaml
series: $SERIES
applications:
  calico:
    options:
      cidr: 10.168.0.0/16
  kubernetes-control-plane:
    options:
      channel: $SNAP_VERSION
      allow-privileged: 'true'
  kubernetes-worker:
    options:
      channel: $SNAP_VERSION
EOF
}

function test::execute
{
    declare -n is_pass=$1

    mkdir -p $HOME/.kube
    juju scp -m $JUJU_CONTROLLER:$JUJU_MODEL kubernetes-control-plane/leader:config $HOME/.kube/
    kubectl version

    # prep azure env
    CK_SEMVER=$(snap find kube-apiserver | grep -o ${CK_VERSION}.[0-9])
    helm uninstall azure-arc || true
    export PATH=${PATH}:$(pwd)
    git clone https://github.com/Azure/azure-arc-validation
    pushd azure-arc-validation/testsuite
    cat <<EOF > ./partner-metadata.md
- Upstream Kubernetes Version: Charmed Kubernetes ${CK_VERSION}
- K8s Distribution Version: v${CK_SEMVER}
- Additional Storage/Network Driver details (if applicable):
- Private Cloud details (if applicable):
- Bare-metal Node details (if applicable):
- OEM/IHV solution details (if applicable):
EOF

    set +x
    source /var/lib/jenkins/.local/share/juju/azure-arc.sh
    if [[ "${UPLOAD_RESULTS}" == "false" ]]; then
      # disable storage account if we don't want to upload results (doesn't impact the test)
      export AZ_STORAGE_ACCOUNT_SAS='invalid'
    else
      # sed treats '&' specially; make sure we escape it if we have any in the key
      export AZ_STORAGE_ACCOUNT_SAS=$(echo ${AZ_STORAGE_ACCOUNT_SAS} | sed -e "s/&/\\\&/g")
    fi
    sed -i \
      -e "s/^az-storage-account=.*/az-storage-account=${AZ_STORAGE_ACCOUNT}/" \
      -e "s/^az-storage-account-sas=.*/az-storage-account-sas='${AZ_STORAGE_ACCOUNT_SAS}'/" \
      -e "s/^offering-name=.*/offering-name=charmed-kubernetes/" \
      -e "s/^global.SUBSCRIPTION_ID=.*/global.SUBSCRIPTION_ID=${AZ_SUBSCRIPTION_ID}/" \
      -e "s/^global.TENANT_ID=.*/global.TENANT_ID=${AZ_TENANT_ID}/" \
      -e "s/^global.RESOURCE_GROUP=.*/global.RESOURCE_GROUP=external-canonical/" \
      -e "s/^global.CLIENT_ID=.*/global.CLIENT_ID=${AZ_CLIENT_ID}/" \
      -e "s/^global.CLIENT_SECRET=.*/global.CLIENT_SECRET=${AZ_CLIENT_SECRET}/" \
      -e "s/^azure-arc-platform.OBJECT_ID=.*/azure-arc-platform.OBJECT_ID='${AZ_OBJECT_ID}'/" \
      azure-arc-conformance.properties
    set -x
    kubectl apply -k .
    # give the arc pod 5 minutes to get settled before tailing its logs
    sleep 300
    ARC_POD=$(kubectl get pods --no-headers -o custom-columns=":metadata.name" -n ${ARC_NS})
    # NB: we always want the logging to succeed so we can watch progress in jenkins;
    # the tail will disconnect when arc tears down (or we timeout)
    timeout -s INT 3h bash -c "kubectl logs -n ${ARC_NS} ${ARC_POD} -f || true"
    ret=$?
    popd

    is_pass="True"
    if (( $ret > 0 )); then
        is_pass="False"
    fi
}

function test::capture
{
    # Arc does 3ish runs; get all of them for our artifacts tarball.
    # NB: strip leading '/' and trailing carriage return since busybox 'find' cant.
    ARC_POD=$(kubectl get pods --no-headers -o custom-columns=":metadata.name" -n ${ARC_NS})
    for res in $(kubectl exec -it ${ARC_POD} -n ${ARC_NS} -- find / -name conformance-results*.gz | sed -e 's|^/||' -e 's|\r$||')
    do
        kubectl cp ${ARC_NS}/${ARC_POD}:${res} ${res} || true
    done

    if which juju-crashdump; then
        juju-crashdump -s -a debug-layer -a config -m "$JUJU_CONTROLLER:$JUJU_MODEL"
    fi
    tar -cvzf artifacts.tar.gz ci.log _out meta juju-crashdump* report.* failures* conformance-results*.gz || true
    python bin/s3 cp "artifacts.tar.gz" artifacts.tar.gz || true

    echo "@@@ CAPTURE RESULTS @@@"
    echo "@"
    echo "@  http://jenkaas.s3-website-us-east-1.amazonaws.com/$JOB_ID/artifacts.tar.gz"
    echo "@"
    echo "@@@"

    rm -rf azure-arc-validation
}


###############################################################################
# ENV
###############################################################################
JUJU_CLOUD=azure/eastus
JUJU_CONTROLLER=arc-ck-$(identifier::short)
JUJU_DEPLOY_BUNDLE=charmed-kubernetes
JUJU_DEPLOY_CHANNEL=${CK_VERSION}/${CK_RISK}
JUJU_MODEL=arc-ck

ARC_NS=azure-arc-kubernetes-conformance
ARCH=amd64
SERIES=focal
SNAP_VERSION=${CK_VERSION}/${CK_RISK}
SONOBUOY_VERSION=0.56.3

JOB_ID=$(identifier)
JOB_NAME_CUSTOM="arc-ck-$SERIES-$SNAP_VERSION"

# Make sure azure repo is available in this env. Doing this manually because
# builders cant curl the auto script from:
# https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt
sudo apt-get install -qyf ca-certificates curl apt-transport-https lsb-release gnupg
cat ./jobs/arc-conformance/microsoft.asc |
    gpg --dearmor |
    sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg > /dev/null
AZ_REPO=$(lsb_release -cs)
echo "deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ ${AZ_REPO} main" |
    sudo tee /etc/apt/sources.list.d/azure-cli.list
sudo apt-get update

###############################################################################
# START
###############################################################################
ci::run
